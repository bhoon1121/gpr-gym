{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:33:13.375907Z",
     "start_time": "2019-03-28T16:33:12.229143Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras.backend as K\n",
    "from glob import glob\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL.ImageOps import invert\n",
    "from keras.utils import plot_model\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session \n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True \n",
    "# dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True # to log device placement (on which device the operation ran) \n",
    "# (nothing gets printed in Jupyter, only if you run it standalone) \n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess) # set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:33:18.286770Z",
     "start_time": "2019-03-28T16:33:13.377065Z"
    }
   },
   "outputs": [],
   "source": [
    "concrete_paths = glob('../train-set/normal/post/*_concrete_*')\n",
    "metallic_paths = glob('../train-set/normal/post/*_pec_*')\n",
    "plastic_paths = glob('../train-set/normal/post/*_pvc_*')\n",
    "real_path = glob('/home/will/data/*.png')\n",
    "\n",
    "def load_img(path):\n",
    "    img = image.load_img(path, grayscale=True, target_size=(256,256), interpolation='hamming')\n",
    "    #img = invert(img)\n",
    "    return image.img_to_array(img)\n",
    "\n",
    "concrete_scans = np.array([load_img(path) for path in concrete_paths])\n",
    "metallic_scans = np.array([load_img(path) for path in metallic_paths])\n",
    "plastic_scans = np.array([load_img(path) for path in plastic_paths])\n",
    "\n",
    "concrete_labels = np.zeros((len(concrete_scans),))\n",
    "metallic_labels = np.ones((len(metallic_scans),))\n",
    "plastic_labels = np.ones((len(plastic_scans),)) * 2\n",
    "\n",
    "y = np.concatenate([concrete_labels, metallic_labels, plastic_labels]).astype(np.int32)\n",
    "a_scans = np.concatenate([concrete_scans, metallic_scans, plastic_scans])\n",
    "\n",
    "print(y.shape)\n",
    "print(a_scans.shape)\n",
    "\n",
    "print(concrete_scans.shape)\n",
    "print(metallic_scans.shape)\n",
    "print(plastic_scans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:33:18.322895Z",
     "start_time": "2019-03-28T16:33:18.288297Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers.merge import _Merge, multiply\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Embedding\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "GRADIENT_PENALTY_WEIGHT = 10 # As per the paper\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "\n",
    "class GPRGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 3\n",
    "        self.latent_dim = 100\n",
    "        self.losslog = []\n",
    "        \n",
    "\n",
    "        self.n_critic = 5\n",
    "        optimizer = Adam(0.0003, 0.5)\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        # Generate image based of noise (fake sample) and add label to the input \n",
    "        label = Input(shape=(1,))\n",
    "        fake_img = self.generator([z_disc, label])\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        averaged_samples = RandomWeightedAverage()([real_img, fake_img])\n",
    "        \n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(averaged_samples)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=averaged_samples)\n",
    "        # Functions need names or Keras will throw an error\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty'\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, label, z_disc],\n",
    "                                  outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                        self.wasserstein_loss,\n",
    "                                        partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        \n",
    "        self.aux_model = self.build_classifier()\n",
    "        \n",
    "        real_pred = self.aux_model(real_img)\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(100,))\n",
    "        # add label to the input\n",
    "        #label = Input(shape=(1,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator([z_gen, label])\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(img)\n",
    "        #gen_pred = self.aux_model(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model([z_gen, label], [valid])\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss,\n",
    "                                     optimizer=optimizer)\n",
    "        \n",
    "        \n",
    "        self.aux_model.compile(loss='sparse_categorical_crossentropy',\n",
    "                               optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        out = Dense(128 * 8 * 8,\n",
    "                        activation=\"relu\",\n",
    "                        input_dim=self.latent_dim)(noise)\n",
    "        out = Reshape((8, 8, 128))(out)\n",
    "        \n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "        out = UpSampling2D()(out)\n",
    "        out = Conv2D(256, kernel_size=5, padding=\"same\")(out)\n",
    "        out = Activation(\"relu\")(out)\n",
    "        \n",
    "        out = BatchNormalization(momentum=0.8)(out)\n",
    "        out = UpSampling2D()(out)\n",
    "        model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(32, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(16, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        \n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, 100)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        gen = Model([noise, label], img)\n",
    "        print(gen.summary())\n",
    "        return gen\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=5, strides=2, padding=\"same\", input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(16*2, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(16*4, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(16*8, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(16*16, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        #model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        \n",
    "        features = model(img)\n",
    "        # Determine validity and label of the image\n",
    "        validity = Dense(1, activation=None)(features)\n",
    "        label_prob = Dense(self.num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "        model = Model(img, validity)\n",
    "        print(model.summary())\n",
    "        return model\n",
    "    \n",
    "    def build_classifier(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=5, strides=2, padding=\"same\", input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Conv2D(32, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(self.num_classes, activation='softmax'))\n",
    "        img = Input(shape=self.img_shape)\n",
    "        \n",
    "        label = model(img)\n",
    "        \n",
    "        return Model(img, label)\n",
    "\n",
    "    def train(self, epochs, batch_size=16, sample_interval=50):\n",
    "        \n",
    "        aux_checkpoint = ModelCheckpoint('checkpoints/aux_model.h5',\n",
    "                                         save_best_only=True)\n",
    "        generator_checkpoint = ModelCheckpoint('checkpoints/generator_freq.h5',\n",
    "                                               save_best_only=True,\n",
    "                                               save_weights_only=True)\n",
    "        disc_checkpoint = ModelCheckpoint('checkpoints/disc_freq.h5',\n",
    "                                          save_best_only=True)\n",
    "        # Load the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(a_scans, y, test_size=0.3)\n",
    "        \n",
    "        \n",
    "\n",
    "        # Configure inputs\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        X_test = (X_test.astype(np.float32) - 127.5) / 127.5\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "        y_test = y_test.reshape(-1, 1)\n",
    "        \n",
    "        np.save('X_test.npy', X_test)\n",
    "        np.save('y_test.npy', y_test)\n",
    "\n",
    "         # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        \n",
    "        test_loss = []\n",
    "            \n",
    "        for epoch in tqdm(range(epochs+1)):\n",
    "            \n",
    "            for _ in range(self.n_critic):\n",
    "                sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "                \n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                test_idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
    "                imgs, labels = X_train[idx], y_train[idx]\n",
    "                test_imgs, test_labels = X_test[test_idx], y_test[test_idx]\n",
    "                \n",
    "                \n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the critic\n",
    "                d_loss = self.critic_model.fit([imgs, labels, noise],\n",
    "                                               [valid, fake, dummy],\n",
    "                                               batch_size=batch_size,\n",
    "                                               validation_data=[[test_imgs, test_labels, noise],\n",
    "                                                                [valid, fake, dummy]],\n",
    "                                               verbose=0,\n",
    "                                               callbacks=[disc_checkpoint])\n",
    "\n",
    "\n",
    "            g_loss = self.generator_model.fit([noise, labels],\n",
    "                                              [valid], \n",
    "                                              batch_size=batch_size,\n",
    "                                              validation_data=[[noise, test_labels],\n",
    "                                                               [valid]],\n",
    "                                              verbose=0,\n",
    "                                              callbacks=[generator_checkpoint])\n",
    "            \n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights(f'generator_normal_{epoch}')\n",
    "\n",
    "            with open('loss_same_labels.log', 'w') as f:\n",
    "                f.writelines('d_loss, g_loss\\n')\n",
    "                for each in self.losslog:\n",
    "                    f.writelines('%s, %s\\n'%(each[0], each[1]))\n",
    "                    \n",
    "        \n",
    "                        \n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 4, 4\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([np.random.randint(0, 3, 1) for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.critic, \"discriminator\")\n",
    "        \n",
    "    def plot_loss(self, gen_loss, disc_loss):\n",
    "        gen_loss = np.array(gen_loss)\n",
    "        disc_loss = np.array(disc_loss)\n",
    "        plt.plot(gen_loss)\n",
    "        plt.savefig('saved_model/gen_loss.png')\n",
    "        plt.plot(disc_loss)\n",
    "        plt.savefig('saved_model/disc_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:33:31.680254Z",
     "start_time": "2019-03-28T16:33:18.324065Z"
    }
   },
   "outputs": [],
   "source": [
    "gpr_gan = GPRGAN()\n",
    "\n",
    "gpr_gan.train(epochs=50000, batch_size=32, sample_interval=500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr-gym",
   "language": "python",
   "name": "gpr-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
