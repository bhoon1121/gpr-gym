{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:09.400678Z",
     "start_time": "2019-01-31T22:01:08.322889Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import tensorflow.keras.backend as K\n",
    "from glob import glob\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL.ImageOps import invert\n",
    "from keras.utils import plot_model\n",
    "\n",
    "np.random.seed(1234)\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "\n",
    "#img = image.load_img(\"/home/will/dev/gpr-learn/Metallic_plastic_none/Metallic 500/metallic98.png\", color_mode='grayscale')\n",
    "#print(img.size)\n",
    "#img = img.resize((256, 256))\n",
    "#test = image.img_to_array(img)[:, :, :]\n",
    "#print(test.shape)\n",
    "#img = image.array_to_img(test)\n",
    "#img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:09.406399Z",
     "start_time": "2019-01-31T22:01:09.402160Z"
    }
   },
   "outputs": [],
   "source": [
    "none_paths = glob('/home/will/dev/gpr-learn/Metallic_plastic_none/None 500/*')\n",
    "metallic_paths = glob('/home/will/dev/gpr-learn/Metallic_plastic_none/Metallic 500/*')\n",
    "plastic_paths = glob('/home/will/dev/gpr-learn/Metallic_plastic_none/Plastic 500/*')\n",
    "\n",
    "def load_img(path):\n",
    "    img = image.load_img(path, color_mode='grayscale')\n",
    "    img = img.resize((256,256))\n",
    "    img = invert(img)\n",
    "    return image.img_to_array(img)\n",
    "\n",
    "none_scans = np.array([load_img(path) for path in none_paths])\n",
    "metallic_scans = np.array([load_img(path) for path in metallic_paths])\n",
    "plastic_scans = np.array([load_img(path) for path in plastic_paths])\n",
    "\n",
    "none_labels = np.zeros((500,))\n",
    "metallic_labels = np.ones((500,))\n",
    "plastic_labels = np.ones((500,)) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:09.413705Z",
     "start_time": "2019-01-31T22:01:09.407612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500,)\n",
      "(0,)\n"
     ]
    }
   ],
   "source": [
    "y = np.concatenate([none_labels, metallic_labels, plastic_labels]).astype(np.int32)\n",
    "a_scans = np.concatenate([none_scans, metallic_scans, plastic_scans])\n",
    "\n",
    "print(y.shape)\n",
    "print(a_scans.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:09.419530Z",
     "start_time": "2019-01-31T22:01:09.415247Z"
    }
   },
   "outputs": [],
   "source": [
    "np.save('a_scans.npy', a_scans)\n",
    "np.save('labels.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:09.451259Z",
     "start_time": "2019-01-31T22:01:09.420885Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers.merge import _Merge, multiply\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Embedding\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from functools import partial\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "\n",
    "class ACWGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.num_classes = 3\n",
    "        self.latent_dim = 100\n",
    "        self.losslog = []\n",
    "        \n",
    "\n",
    "        self.n_critic = 5\n",
    "        #optimizer = RMSprop(lr=0.00005)\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "        # Build the generator and critic\n",
    "        self.generator = self.build_generator()\n",
    "        self.critic = self.build_critic()\n",
    "\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "\n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        \n",
    "        # Generate image based of noise (fake sample) and add label to the input \n",
    "        label = Input(shape=(1,))\n",
    "        fake_img = self.generator([z_disc, label])\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake, _ = self.critic(fake_img)\n",
    "        valid, label_prob = self.critic(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        \n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated, _ = self.critic(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, label, z_disc], outputs=[valid, fake, validity_interpolated, label_prob])\n",
    "        self.critic_model.compile(loss=[self.wasserstein_loss,\n",
    "                                        self.wasserstein_loss,\n",
    "                                        partial_gp_loss,\n",
    "                                        'sparse_categorical_crossentropy'],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 1, 10],\n",
    "                                        metrics=['accuracy'])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(100,))\n",
    "        # add label to the input\n",
    "        #label = Input(shape=(1,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator([z_gen, label])\n",
    "        # Discriminator determines validity\n",
    "        valid, label_pred = self.critic(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model([z_gen, label], [valid, label_pred])\n",
    "        self.generator_model.compile(loss=[self.wasserstein_loss,\n",
    "                                          'sparse_categorical_crossentropy'],\n",
    "                                     optimizer=optimizer)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "    \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 8 * 8, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((8, 8, 128)))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(256, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(32, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(16, kernel_size=5, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        \n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding='same'))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(1,), dtype='int32')\n",
    "        label_embedding = Flatten()(Embedding(self.num_classes, 100)(label))\n",
    "\n",
    "        model_input = multiply([noise, label_embedding])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "\n",
    "    def build_critic(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(16, kernel_size=5, strides=2, padding=\"same\", input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(32, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(64, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=5, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.3))\n",
    "        #model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        #model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        \n",
    "        features = model(img)\n",
    "        # Determine validity and label of the image\n",
    "        validity = Dense(1, activation=None)(features)\n",
    "        label_prob = Dense(self.num_classes, activation=\"softmax\")(features)\n",
    "\n",
    "        return Model(img, [validity, label_prob])\n",
    "\n",
    "    def train(self, epochs, batch_size=16, sample_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train, X_test, y_train, y_test = train_test_split(a_scans, y, test_size=0.3)\n",
    "\n",
    "        # Configure inputs\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "         # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        \n",
    "        for epoch in range(epochs+1):\n",
    "            for _ in range(self.n_critic):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                imgs, labels = X_train[idx], y_train[idx]\n",
    "                # Sample generator input\n",
    "                noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "                # Train the critic\n",
    "                d_loss = self.critic_model.train_on_batch([imgs, labels, noise], [valid, fake, dummy, labels])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            sampled_labels = np.random.randint(0, self.num_classes, batch_size).reshape(-1, 1)\n",
    "            g_loss = self.generator_model.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            \n",
    "            self.losslog.append([d_loss[0], g_loss])\n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                print (\"%d [D loss: %f, acc.: %.2f, op_acc: %.2f%%] [G loss: %f]\" % (\n",
    "                epoch, d_loss[0], d_loss[3], 100*d_loss[-1], g_loss))\n",
    "                #print(\"Evaluating Model...\")\n",
    "                #idx = np.random.randint(0, X_test.shape[0], batch_size)\n",
    "                #imgs, labels = X_test[idx], y_test[idx]\n",
    "                #d_loss = self.critic_model.eval_on_batch([imgs, labels, noise], [valid, fake, dummy, labels])\n",
    "                #print (\"%d [D loss: %f] [Aux loss: %f]\" % (epoch, d_loss[0], d_loss[-1]))\n",
    "                self.sample_images(epoch)\n",
    "                self.generator.save_weights('generator_same_labels', overwrite=True)\n",
    "                self.critic.save_weights('discriminator_same_labels', overwrite=True)\n",
    "                with open('loss_same_labels.log', 'w') as f:\n",
    "                    f.writelines('d_loss, g_loss\\n')\n",
    "                    for each in self.losslog:\n",
    "                        f.writelines('%s, %s\\n'%(each[0], each[1]))\n",
    "                        \n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 2, 2\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        sampled_labels = np.array([num for _ in range(r) for num in range(c)])\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray_r')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.critic, \"discriminator\")\n",
    "        \n",
    "    def plot_loss(self, gen_loss, disc_loss):\n",
    "        gen_loss = np.array(gen_loss)\n",
    "        disc_loss = np.array(disc_loss)\n",
    "        plt.plot(gen_loss)\n",
    "        plt.savefig('saved_model/gen_loss.png')\n",
    "        plt.plot(disc_loss)\n",
    "        plt.savefig('saved_model/disc_loss.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:12.411144Z",
     "start_time": "2019-01-31T22:01:09.453085Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 1500]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-566a0d6c720a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0macgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mACWGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0macgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-2bf9185a225d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_scans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;31m# Configure inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/gpr-gym/lib/python3.6/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(*arrays, **options)\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/gpr-gym/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/gpr-gym/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 1500]"
     ]
    }
   ],
   "source": [
    "acgan = ACWGAN()\n",
    "\n",
    "acgan.train(epochs=50000, batch_size=32, sample_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:12.412752Z",
     "start_time": "2019-01-31T22:01:08.328Z"
    }
   },
   "outputs": [],
   "source": [
    "model = ACWGAN().build_generator()\n",
    "plot_model(model, to_file='model.png')\n",
    "#model.load_weights('generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-31T22:01:12.413621Z",
     "start_time": "2019-01-31T22:01:08.330Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "noise = np.random.normal(0, 1, (1, 100))\n",
    "sampled_labels = np.array([2])\n",
    "print(sampled_labels)\n",
    "gen_imgs = model.predict([noise, sampled_labels])\n",
    "gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "gen_imgs.shape\n",
    "invert(image.array_to_img(np.squeeze(gen_imgs, 0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpr-gym",
   "language": "python",
   "name": "gpr-gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
